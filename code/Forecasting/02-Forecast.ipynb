{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2504f63c-d738-4acc-bd5c-b025e6f3805b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 02 Forecast\n",
    "- Import necessary data for prediction\n",
    "- Feature creation\n",
    "- Perform forecasts\n",
    "- Export prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c81a96-baea-4ef1-8d23-d941b4938dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting flaml\n  Using cached FLAML-2.3.2-py3-none-any.whl (313 kB)\nRequirement already satisfied: NumPy>=1.17 in /databricks/python3/lib/python3.10/site-packages (from flaml) (1.23.5)\nInstalling collected packages: flaml\nSuccessfully installed flaml-2.3.2\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install flaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77a18fc-4dd3-40c2-8a01-fe12d89b548e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba4018fe-f0fe-4d2b-8821-3f667d76a179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from flaml import AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "612bf074-91fd-4bde-bea1-922eb86c6465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing data necessary for prediction\n",
    "input_path = '/dbfs/mnt/02_SILVER/Growth_Navigator_STRATIS/hf/'\n",
    "input_path_sj = '/dbfs/mnt/02_SILVER/Growth_Navigator_STRATIS/sj/'\n",
    "\n",
    "filename = 'data_for_prediction_sj.csv'\n",
    "data = pd.read_csv(input_path_sj + filename,  parse_dates=['SES_TRX_DATE'])\n",
    "\n",
    "custom_scaler = pd.read_csv(input_path + 'custom_scaler_sj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67efb6fa-2250-429f-aeb1-05ac06be1ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns\n",
    "data.drop(columns=['month_1', 'month_2', 'month_3','month_4', 'month_5', 'month_6','month_7', 'month_8', 'month_9','month_10', 'month_11', 'month_12', \n",
    "                   'dayofweek_0', 'dayofweek_1','dayofweek_2', 'dayofweek_3', 'dayofweek_4', 'dayofweek_5','dayofweek_6',\n",
    "                   'year_2023',  'year_2024'], inplace=True)\n",
    "\n",
    "data.drop(columns=['VOUCHER_FLAG'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d108812-d60e-4f2d-acf3-dd12d1e34425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create features\n",
    "- Using past data as features (lag features) for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9058bd0-a2de-4f30-b43b-6c7a52257c1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating the features used for prediction\n",
    "def create_features(data, n_lags=7, feature_name=None, horizont=7):\n",
    "    features = data.sort_values(['id', 'SES_TRX_DATE'])\n",
    "    features.drop(columns=[feature_name], inplace=True)\n",
    "\n",
    "    # Create lag features\n",
    "    for lag in range(horizont, 2*horizont+1, 1):\n",
    "        features[f'{feature_name}_lag{lag}'] = data[['id', f'{feature_name}']].groupby('id').shift(lag)\n",
    "\n",
    "    for lag in range(2*horizont, 8*horizont + 1, 7):\n",
    "        features[f'{feature_name}_lag{lag}'] = data[['id', f'{feature_name}']].groupby('id').shift(lag)\n",
    "\n",
    "    for lag in range(8*horizont, 26*horizont + 1, 28):\n",
    "        features[f'{feature_name}_lag{lag}'] = data[['id', f'{feature_name}']].groupby('id').shift(lag)\n",
    "    \n",
    "    features[feature_name] = data[f'{feature_name}']\n",
    "    feature_list = features.columns.tolist()\n",
    "\n",
    "    return features, feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2924c510-1969-4309-a185-5c65bedd7bbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Forecast function\n",
    "- Define LGBM forecast function\n",
    "- Perform predictions\n",
    "- Calculate MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d57c8d-1683-4d34-918b-869257313ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining parameters for the prediction\n",
    "# The target variable we want to predict\n",
    "target = 'value_scaled'\n",
    "\n",
    "# The start date for the predictions\n",
    "prediction_start_date = '2024-10-01'\n",
    "\n",
    "# Confidence level for the lower/upper bound\n",
    "alpha = 0.7\n",
    "\n",
    "# Horizont of the predictions\n",
    "horizont = 7\n",
    "\n",
    "# Preparing data\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.loc[data.SES_TRX_DATE >= prediction_start_date, target] = None\n",
    "\n",
    "# Creating features\n",
    "data_all, feature_list = create_features(data, \n",
    "                                         n_lags= 2*horizont, \n",
    "                                         feature_name=target, \n",
    "                                         horizont=horizont)\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "       'objective': 'regression',  # Multivariate regression task\n",
    "       'metric': 'mape',\n",
    "       'boosting_type': 'gbdt',  # Gradient boosting decision tree\n",
    "       'learning_rate': 0.05,\n",
    "       'num_leaves': 100, #31\n",
    "       'feature_fraction': 0.9,\n",
    "       'bagging_fraction': 0.8,\n",
    "       'bagging_freq': 5\n",
    "}\n",
    "\n",
    "# Alternative parameters for quantile regression\n",
    "lower_params = {\n",
    "    'objective': 'quantile',  \n",
    "    'alpha': 1 - alpha, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 100,  # 31\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "}\n",
    "\n",
    "upper_params = {\n",
    "    'objective': 'quantile',  \n",
    "    'alpha': alpha, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 100,  # 31\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce7e0ed-3fd5-4837-9235-d10126af3338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Finding the start and end of the prediction period\n",
    "prediction_start_date = pd.to_datetime(prediction_start_date)\n",
    "prediction_end_date = pd.to_datetime(prediction_start_date) + pd.offsets.MonthEnd(0)\n",
    "  \n",
    "# Creating the list of dates for prediction\n",
    "dates = pd.date_range(start=prediction_start_date, end=prediction_end_date, freq='7D')\n",
    "dates = dates.append(pd.DatetimeIndex([prediction_end_date])+pd.Timedelta(days=1))\n",
    "\n",
    "# Define prediction intervals\n",
    "intervals = [(dates[i], dates[i + 1] - pd.Timedelta(days=1)) for i in range(len(dates) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4be83cb-9a73-4aa5-a267-63951d6ee57f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining the function for the prediction\n",
    "def predict_future(data, prediction_start_date, prediction_end_date, target, horizont):\n",
    "\n",
    "  # Separating past and future data\n",
    "  dt_past = data[data.SES_TRX_DATE < prediction_start_date].dropna()\n",
    "  dt_future = data[(data.SES_TRX_DATE >= prediction_start_date) & (data.SES_TRX_DATE <= prediction_end_date)]      \n",
    "\n",
    "  # Separating predictors and target \n",
    "  X = dt_past.drop(columns=['value', 'id', 'SES_TRX_DATE', target])\n",
    "  y = dt_past[target]  \n",
    "\n",
    "  # Split data into training and test sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "  # Create LightGBM dataset\n",
    "  train_data = lgb.Dataset(X_train, label=y_train)\n",
    "  test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "  # Train the main regression model\n",
    "  model = lgb.train(\n",
    "      params,\n",
    "      train_data,\n",
    "      valid_sets=[train_data, test_data],\n",
    "      num_boost_round=100,\n",
    "      callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    "  )\n",
    "\n",
    "  # Train lower quantile model\n",
    "  lower = lgb.train(\n",
    "      lower_params,\n",
    "      train_data,\n",
    "      valid_sets=[train_data, test_data],\n",
    "      num_boost_round=100,\n",
    "      callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    "  )\n",
    "\n",
    "  # Train upper quantile model\n",
    "  upper = lgb.train(\n",
    "      upper_params,\n",
    "      train_data,\n",
    "      valid_sets=[train_data, test_data],\n",
    "      num_boost_round=100,\n",
    "      callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    "  )\n",
    "\n",
    "  # Calculate contributions and predictions\n",
    "  contributions = model.predict(\n",
    "      dt_future.drop(columns=['value', 'id', 'SES_TRX_DATE', target]),\n",
    "      num_iteration=model.best_iteration,\n",
    "      pred_contrib=True\n",
    "  )\n",
    "\n",
    "  dt_future['predicted'] = model.predict(\n",
    "      dt_future.drop(columns=['value', 'id', 'SES_TRX_DATE', target]),\n",
    "      num_iteration=model.best_iteration, predict_disable_shape_check=True\n",
    "  )\n",
    "  dt_future['lower_scaled'] = lower.predict(\n",
    "      dt_future.drop(columns=['value', 'id', 'SES_TRX_DATE', target]),\n",
    "      num_iteration=model.best_iteration, predict_disable_shape_check=True\n",
    "  )\n",
    "  dt_future['upper_scaled'] = upper.predict(\n",
    "      dt_future.drop(columns=['value', 'id', 'SES_TRX_DATE', target]),\n",
    "      num_iteration=model.best_iteration, predict_disable_shape_check=True\n",
    "  )\n",
    "\n",
    "  # Apply custom scaling\n",
    "  dt_future = dt_future.merge(custom_scaler, how='left', on=['id'])\n",
    "  dt_future['prediction'] = dt_future.apply(lambda x: max(0, x['predicted'] * (x['max'] - x['min']) + x['min']), axis=1)\n",
    "  dt_future['lower_bound'] = dt_future.apply(lambda x: max(0, x['lower_scaled'] * (x['max'] - x['min']) + x['min']), axis=1)\n",
    "  dt_future['upper_bound'] = dt_future.apply(lambda x: min(max(0, x['upper_scaled'] * (x['max'] - x['min']) + x['min']), x['max'] * 1.5), axis=1)\n",
    "\n",
    "  # Add the prediction horizon\n",
    "  dt_future['horizont'] = horizont\n",
    "\n",
    "  return dt_future[['id', 'SES_TRX_DATE', 'horizont', 'prediction', 'lower_bound', 'upper_bound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ab6a2f-cbbc-4604-ad8f-0471cdca1b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizont = 7\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007634 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5367\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 280\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 0.437076\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's mape: 0.0723902\tvalid_1's mape: 0.0770645\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037728 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5367\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 280\n[LightGBM] [Info] Start training from score 0.339623\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's quantile: 0.0310811\tvalid_1's quantile: 0.0330598\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029970 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5367\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 280\n[LightGBM] [Info] Start training from score 0.533994\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's quantile: 0.0325642\tvalid_1's quantile: 0.0347451\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/71994/command-823817645982297-2034238233:53: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['predicted'] = model.predict(\n/root/.ipykernel/71994/command-823817645982297-2034238233:57: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['lower_scaled'] = lower.predict(\n/root/.ipykernel/71994/command-823817645982297-2034238233:61: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['upper_scaled'] = upper.predict(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizont = 14\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022815 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3582\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 273\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 0.437076\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's mape: 0.0751022\tvalid_1's mape: 0.081259\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028785 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3582\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 273\n[LightGBM] [Info] Start training from score 0.339623\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's quantile: 0.0322717\tvalid_1's quantile: 0.03494\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005317 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3582\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 273\n[LightGBM] [Info] Start training from score 0.533994\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's quantile: 0.0337659\tvalid_1's quantile: 0.0364243\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/71994/command-823817645982297-2034238233:53: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['predicted'] = model.predict(\n/root/.ipykernel/71994/command-823817645982297-2034238233:57: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['lower_scaled'] = lower.predict(\n/root/.ipykernel/71994/command-823817645982297-2034238233:61: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['upper_scaled'] = upper.predict(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizont = 21\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028713 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3327\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 272\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 0.437076\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's mape: 0.0765381\tvalid_1's mape: 0.08361\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005856 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3327\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 272\n[LightGBM] [Info] Start training from score 0.339623\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's quantile: 0.0327888\tvalid_1's quantile: 0.0360865\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007644 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3327\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 272\n[LightGBM] [Info] Start training from score 0.533994\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's quantile: 0.0344435\tvalid_1's quantile: 0.0374811\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/71994/command-823817645982297-2034238233:53: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['predicted'] = model.predict(\n/root/.ipykernel/71994/command-823817645982297-2034238233:57: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['lower_scaled'] = lower.predict(\n/root/.ipykernel/71994/command-823817645982297-2034238233:61: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['upper_scaled'] = upper.predict(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizont = 28\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005084 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3072\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 271\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 0.437076\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's mape: 0.0777902\tvalid_1's mape: 0.0858443\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026815 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3072\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 271\n[LightGBM] [Info] Start training from score 0.339623\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's quantile: 0.0333365\tvalid_1's quantile: 0.0369885\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017644 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3072\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 271\n[LightGBM] [Info] Start training from score 0.533994\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's quantile: 0.0350185\tvalid_1's quantile: 0.0383396\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/71994/command-823817645982297-2034238233:53: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['predicted'] = model.predict(\n/root/.ipykernel/71994/command-823817645982297-2034238233:57: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['lower_scaled'] = lower.predict(\n/root/.ipykernel/71994/command-823817645982297-2034238233:61: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['upper_scaled'] = upper.predict(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horizont = 31\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004818 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2817\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 270\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 0.437076\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[99]\ttraining's mape: 0.0792624\tvalid_1's mape: 0.0877647\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004713 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2817\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 270\n[LightGBM] [Info] Start training from score 0.339623\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's quantile: 0.0338705\tvalid_1's quantile: 0.0379245\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022955 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2817\n[LightGBM] [Info] Number of data points in the train set: 651522, number of used features: 270\n[LightGBM] [Info] Start training from score 0.533994\nTraining until validation scores don't improve for 100 rounds\nDid not meet early stopping. Best iteration is:\n[100]\ttraining's quantile: 0.0356902\tvalid_1's quantile: 0.0391672\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/71994/command-823817645982297-2034238233:53: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['predicted'] = model.predict(\n/root/.ipykernel/71994/command-823817645982297-2034238233:57: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['lower_scaled'] = lower.predict(\n/root/.ipykernel/71994/command-823817645982297-2034238233:61: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dt_future['upper_scaled'] = upper.predict(\n"
     ]
    }
   ],
   "source": [
    "# Performing the prediction for the intervals\n",
    "prediction_all = pd.DataFrame() \n",
    "\n",
    "for i in intervals:\n",
    "  # Get the start and end date for the prediction\n",
    "  start = pd.to_datetime(i[0])\n",
    "  end = pd.to_datetime(i[1])\n",
    "  h = (end - pd.to_datetime(prediction_start_date)).days + 1 \n",
    "  \n",
    "  # Get the feature list\n",
    "  features_wo_lag = [f for f in feature_list if \"lag\" not in f]\n",
    "  features_w_lag = [f for f in feature_list if \"lag\" in f and int(f.split('lag')[-1]) >= h]\n",
    "  features = features_wo_lag + features_w_lag\n",
    "\n",
    "  df = data_all[features]\n",
    "  print(f\"horizont = {h}\")\n",
    "\n",
    "  # Prediction\n",
    "  prediction = predict_future(df, start, end, target, horizont=h)\n",
    "  prediction_all = pd.concat([prediction_all, prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff1388b-2e9f-45eb-8957-1ab458c6b0ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.2160614426051421"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Calculate MAPE fpr all predictions\n",
    "dt_plot = data_all[['SES_TRX_DATE', 'id', 'value']].merge(prediction_all, how='left', on = ['id', 'SES_TRX_DATE'])\n",
    "a = dt_plot[(~dt_plot.prediction.isna())&(dt_plot.value > 0)]\n",
    "\n",
    "mean_absolute_percentage_error(a['value'], a['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b99eee7-5902-47a6-b992-a203d8b395ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Export predictions\n",
    "prediction_all.to_csv(input_path_sj+f'pred/lightgbm_4weeks_pred_{prediction_start_date}_eval_bound{alpha}_old.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9107b7f3-b9c6-482b-882a-9296d8df635f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "horizont\n",
       "7.0     0.208862\n",
       "14.0    0.203308\n",
       "21.0    0.192915\n",
       "28.0    0.258383\n",
       "31.0    0.217832\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate MAPE for each horizont\n",
    "a.groupby('horizont').apply(lambda x: mean_absolute_percentage_error(x['value'], x['prediction']))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02-Forecast",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}